name: GPU Performance Monitoring

on:
  schedule:
    # Run performance tests weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'full'
        type: choice
        options:
        - quick
        - full
        - matrix-only
        - memory-only

env:
  PYTHON_VERSION: '3.12'

jobs:
  # GPU library compatibility check
  gpu-compatibility:
    runs-on: self-hosted # Requires GPU runner
    name: GPU Compatibility Check
    if: contains(github.event.inputs.benchmark_type, 'full') || github.event_name == 'schedule'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install GPU libraries
        run: |
          python -m pip install --upgrade pip
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
          pip install cupy-cuda12x
          pip install numba==0.60.0

      - name: Check GPU availability
        run: |
          nvidia-smi
          python -c "
          import torch
          print(f'PyTorch CUDA available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU: {torch.cuda.get_device_name(0)}')
              print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "

      - name: Run comprehensive GPU test
        run: |
          python gpu-libraries-comprehensive-test.py > gpu-test-results.txt

      - name: Upload GPU test results
        uses: actions/upload-artifact@v3
        with:
          name: gpu-compatibility-results
          path: gpu-test-results.txt

  # Performance benchmarking
  performance-benchmark:
    runs-on: self-hosted # Requires GPU runner
    name: Performance Benchmarking
    needs: gpu-compatibility
    strategy:
      matrix:
        benchmark: [vector-operations, matrix-multiplication, memory-transfer, catalytic-lattice]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run specific benchmark
        run: |
          case "${{ matrix.benchmark }}" in
            "vector-operations")
              python gpu-performance-benchmark.py --test vector > vector-benchmark.json
              ;;
            "matrix-multiplication")
              python matrix-optimization-benchmark.py > matrix-benchmark.json
              ;;
            "memory-transfer")
              python gpu-performance-benchmark.py --test memory > memory-benchmark.json
              ;;
            "catalytic-lattice")
              python apps/catalytic/test_lattice_integration_suite.py > lattice-benchmark.json
              ;;
          esac

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: performance-benchmark-${{ matrix.benchmark }}
          path: "*-benchmark.json"

  # Memory efficiency testing
  memory-efficiency:
    runs-on: self-hosted # Requires GPU runner
    name: Memory Efficiency Analysis
    if: contains(github.event.inputs.benchmark_type, 'memory') || contains(github.event.inputs.benchmark_type, 'full') || github.event_name == 'schedule'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install psutil gpustat

      - name: Run memory efficiency analysis
        run: |
          python apps/catalytic/memory_optimization_analyzer.py > memory-analysis.json

      - name: Monitor GPU memory usage
        run: |
          gpustat --json > gpu-memory-stats.json

      - name: Upload memory analysis
        uses: actions/upload-artifact@v3
        with:
          name: memory-efficiency-analysis
          path: |
            memory-analysis.json
            gpu-memory-stats.json

  # Performance regression detection
  regression-detection:
    runs-on: ubuntu-latest
    name: Performance Regression Detection
    needs: [performance-benchmark]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-results/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install analysis tools
        run: |
          pip install pandas matplotlib seaborn

      - name: Analyze performance trends
        run: |
          python -c "
          import json
          import os
          import glob
          
          print('=== Performance Regression Analysis ===')
          
          # Define performance thresholds
          thresholds = {
              'pytorch_tflops': 6.0,      # Minimum 6 TFLOPS
              'cupy_tflops': 6.0,         # Minimum 6 TFLOPS  
              'numba_gops': 5.0,          # Minimum 5 GOPS
              'memory_efficiency': 100.0  # Minimum 100x reduction
          }
          
          # Check if we have baseline results to compare against
          baseline_file = 'baseline-performance.json'
          if os.path.exists(baseline_file):
              with open(baseline_file, 'r') as f:
                  baseline = json.load(f)
              print('Baseline performance data loaded')
          else:
              print('No baseline found - this will become the new baseline')
              baseline = {}
          
          # Process current benchmark results
          results_files = glob.glob('benchmark-results/**/*.json', recursive=True)
          print(f'Found {len(results_files)} result files')
          
          # Simple regression check
          regression_detected = False
          for threshold_name, min_value in thresholds.items():
              print(f'Checking {threshold_name} >= {min_value}')
              # This would be expanded with actual result parsing
          
          if regression_detected:
              print('❌ Performance regression detected!')
              exit(1)
          else:
              print('✅ No performance regression detected')
          "

  # Generate performance report
  performance-report:
    runs-on: ubuntu-latest
    name: Generate Performance Report
    needs: [performance-benchmark, memory-efficiency, regression-detection]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v3
        with:
          path: all-results/

      - name: Generate performance report
        run: |
          echo "# 🚀 GPU Performance Report" > performance-report.md
          echo "**Date**: $(date -u)" >> performance-report.md
          echo "**Commit**: ${{ github.sha }}" >> performance-report.md
          echo "" >> performance-report.md
          
          echo "## 📊 Benchmark Results" >> performance-report.md
          echo "- GPU Compatibility: ${{ needs.gpu-compatibility.result }}" >> performance-report.md
          echo "- Performance Benchmark: ${{ needs.performance-benchmark.result }}" >> performance-report.md
          echo "- Memory Efficiency: ${{ needs.memory-efficiency.result }}" >> performance-report.md
          echo "- Regression Detection: ${{ needs.regression-detection.result }}" >> performance-report.md
          echo "" >> performance-report.md
          
          echo "## 🎯 Key Metrics" >> performance-report.md
          echo "- Target PyTorch Performance: ≥6.0 TFLOPS" >> performance-report.md
          echo "- Target CuPy Performance: ≥6.0 TFLOPS" >> performance-report.md
          echo "- Target Memory Efficiency: ≥100x reduction" >> performance-report.md
          echo "- Target Catalytic Speedup: ≥649x parallel" >> performance-report.md
          echo "" >> performance-report.md
          
          echo "## 📈 Recommendations" >> performance-report.md
          if [ "${{ needs.performance-benchmark.result }}" == "success" ]; then
            echo "✅ GPU performance is optimal" >> performance-report.md
          else
            echo "⚠️ GPU performance needs investigation" >> performance-report.md
          fi

      - name: Upload performance report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: performance-report.md

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  # Archive results for trending analysis
  archive-results:
    runs-on: ubuntu-latest
    name: Archive Results
    needs: [performance-report]
    if: github.event_name == 'schedule' # Only archive scheduled runs
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v3
        with:
          path: archive-results/

      - name: Create archive entry
        run: |
          mkdir -p performance-history/$(date +%Y-%m)
          cp -r archive-results/* performance-history/$(date +%Y-%m)/
          echo "$(date -u): ${{ github.sha }}" >> performance-history/timeline.txt

      - name: Commit performance history
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add performance-history/
          git commit -m "Archive performance results for $(date +%Y-%m-%d)" || exit 0
          git push