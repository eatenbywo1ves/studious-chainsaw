groups:
  - name: catalytic.rules
    rules:
    # High-level service availability
    - alert: ServiceDown
      expr: up == 0
      for: 5m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Service {{ $labels.job }} is down"
        description: "{{ $labels.job }} has been down for more than 5 minutes."
        runbook_url: "https://runbook.catalytic.dev/alerts/service-down"

    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
      for: 2m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}"
        runbook_url: "https://runbook.catalytic.dev/alerts/high-error-rate"

    # Response time alerts
    - alert: HighResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2.0
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}"

    - alert: CriticalResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5.0
      for: 2m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Critical response time detected"
        description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}"

    # Database alerts
    - alert: DatabaseConnectionsHigh
      expr: pg_stat_activity_count > 80
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High number of database connections"
        description: "Database has {{ $value }} active connections"

    - alert: DatabaseDown
      expr: pg_up == 0
      for: 1m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Database is down"
        description: "PostgreSQL database is not responding"
        runbook_url: "https://runbook.catalytic.dev/alerts/database-down"

    - alert: DatabaseSlowQueries
      expr: rate(pg_stat_statements_mean_time_seconds[5m]) > 1.0
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Slow database queries detected"
        description: "Average query time is {{ $value }}s"

    # Memory and CPU alerts
    - alert: HighMemoryUsage
      expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.85
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High memory usage"
        description: "Memory usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"

    - alert: CriticalMemoryUsage
      expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.95
      for: 2m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Critical memory usage"
        description: "Memory usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"

    - alert: HighCPUUsage
      expr: rate(container_cpu_usage_seconds_total[5m]) > 0.8
      for: 10m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High CPU usage"
        description: "CPU usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"

    # Redis alerts
    - alert: RedisDown
      expr: redis_up == 0
      for: 1m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Redis is down"
        description: "Redis cache is not responding"

    - alert: RedisHighMemoryUsage
      expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Redis memory usage is high"
        description: "Redis memory usage is {{ $value | humanizePercentage }}"

    # Application-specific alerts
    - alert: StripeWebhookFailures
      expr: rate(stripe_webhook_failures_total[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High Stripe webhook failure rate"
        description: "Stripe webhook failure rate is {{ $value }}/sec"

    - alert: EmailDeliveryFailures
      expr: rate(email_delivery_failures_total[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "High email delivery failure rate"
        description: "Email delivery failure rate is {{ $value }}/sec"

    - alert: SubscriptionSyncFailures
      expr: rate(subscription_sync_failures_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Subscription sync failures detected"
        description: "Subscription synchronization is failing"

    # SSL Certificate expiry
    - alert: SSLCertificateExpiry
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
      for: 1h
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "SSL certificate expires soon"
        description: "SSL certificate for {{ $labels.instance }} expires in {{ $value | humanizeDuration }}"

    - alert: SSLCertificateExpiryUrgent
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7
      for: 1h
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "SSL certificate expires very soon"
        description: "SSL certificate for {{ $labels.instance }} expires in {{ $value | humanizeDuration }}"

    # Kubernetes cluster alerts
    - alert: KubernetesPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"

    - alert: KubernetesNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Kubernetes node not ready"
        description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

    - alert: KubernetesPodNotRunning
      expr: kube_pod_status_phase{phase!="Running"} > 0
      for: 10m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Pod not running"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not running"

    # Disk space alerts
    - alert: DiskSpaceHigh
      expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.2
      for: 5m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Disk space is running low"
        description: "Disk space usage is {{ $value | humanizePercentage }} on {{ $labels.device }}"

    - alert: DiskSpaceCritical
      expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
      for: 1m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Disk space critically low"
        description: "Disk space usage is {{ $value | humanizePercentage }} on {{ $labels.device }}"

    # Load balancer alerts
    - alert: LoadBalancerDown
      expr: probe_success{job="blackbox-http"} == 0
      for: 2m
      labels:
        severity: critical
        environment: production
      annotations:
        summary: "Load balancer health check failed"
        description: "Load balancer health check failed for {{ $labels.instance }}"

    # Blue-Green deployment monitoring
    - alert: BlueGreenEnvironmentMismatch
      expr: count(up{environment="blue"}) != count(up{environment="green"}) and count(up{environment="blue"}) > 0 and count(up{environment="green"}) > 0
      for: 15m
      labels:
        severity: warning
        environment: production
      annotations:
        summary: "Blue-Green environment mismatch"
        description: "Different number of services running in blue vs green environments"

  - name: catalytic.staging.rules
    rules:
    # Staging-specific alerts with relaxed thresholds
    - alert: StagingServiceDown
      expr: up{environment="staging"} == 0
      for: 10m
      labels:
        severity: warning
        environment: staging
      annotations:
        summary: "Staging service {{ $labels.job }} is down"
        description: "{{ $labels.job }} has been down for more than 10 minutes in staging"

    - alert: StagingHighErrorRate
      expr: rate(http_requests_total{status=~"5..",environment="staging"}[5m]) / rate(http_requests_total{environment="staging"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        environment: staging
      annotations:
        summary: "High error rate in staging"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }} in staging"